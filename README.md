## Realtime AI Backend (FastAPI + Supabase + Groq)

This assignment-ready backend demonstrates a realtime conversational stack featuring WebSockets, LangGraph-orchestrated Groq interactions, Supabase persistence, and async post-session automation. The server streams Groq tokens to clients in near‑realtime while logging every step to Supabase for later automation.

### Architecture at a Glance
- **FastAPI WebSocket endpoint** `ws://host/ws/session/{session_id}` accepts JSON or raw text payloads and streams response tokens back as they are produced.
- **LangGraph router** classifies each user turn, optionally executing a simulated internal tool (`knowledge_base_lookup`) before shaping the final system prompt sent to Groq through LangChain.
- **Supabase Postgres** stores a `sessions` record plus detailed `session_events` for each user/AI turn, enabling analytics and the post-session automation step.
- **Post-session automation** (triggered on disconnect) replays event logs, summarizes the full conversation via Groq, and finalizes the session record with summary + end timestamps.

### Setup
```powershell
# 1. Create and activate a virtual environment (PowerShell)
cd D:\TECNVIRONS
python -m venv .venv
.venv\Scripts\Activate.ps1

# 2. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# 3. Provide required secrets
copy env.example .env  # then edit values
```

#### Required environment variables
| Name | Description |
| --- | --- |
| `GROQ_API_KEY` | Your Groq API key |
| `GROQ_MODEL` | Optional override, defaults to `llama-3.1-70b-versatile` |
| `SUPABASE_URL` | Supabase project URL |
| `SUPABASE_SERVICE_ROLE_KEY` | Service role key with insert/update permissions |
| `SUPABASE_SCHEMA` | Optional schema name (defaults to `public`) |

`env.example` inside the repo already mirrors the variables above for quick sharing.

### Supabase Schema
```sql
create table public.sessions (
  session_id uuid primary key,
  user_id text not null,
  status text not null default 'active',
  start_time timestamptz default now(),
  end_time timestamptz,
  summary text
);

create table public.session_events (
  id bigint generated by default as identity primary key,
  session_id uuid not null references public.sessions(session_id) on delete cascade,
  event_type text not null,
  payload jsonb not null,
  occurred_at timestamptz not null default now()
);

create index session_events_session_idx on public.session_events(session_id, occurred_at);
```

### Running the WebSocket Server
```powershell
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```
1. Connect via WebSocket: `ws://localhost:8000/ws/session/<uuid>?user_id=alice`
2. Send either raw text or `{ "message": "your text" }`.
3. Receive streaming tokens until `[END_OF_RESPONSE]`.

### Live Demo

- **Backend (Render)**: `https://tecnvirons.onrender.com`  
  - Health: `https://tecnvirons.onrender.com/healthz`  
  - WebSocket base: `wss://tecnvirons.onrender.com/ws/session/{session_id}?user_id={user_id}`
- **Frontend (Vercel)**: `https://tecnvirons-git-master-sankalp-singhs-projects-08580eee.vercel.app/`

### Basic HTML Frontend (local)
Located in `frontend/` and intentionally framework-free.

```powershell
# from repo root, after backend is running
cd frontend
# use any static server (examples):
python -m http.server 4173
# or open index.html directly in a browser (no build step required)
```

Once opened:
1. For local dev keep `ws://localhost:8000`; in production with Render use `wss://tecnvirons.onrender.com`.
2. Generate a session ID (or paste your own) and click **Connect**.
3. Type a prompt. The UI streams AI tokens live; `[END_OF_RESPONSE]` is handled automatically.
4. Use **Disconnect** to trigger the backend’s post-session summary.

### Deploying on Render (backend) + Vercel (frontend)
- **Backend (Render)**: the repo includes `render.yaml`. On Render, create a new Web Service from the GitHub repo, set the environment to Python, and add env vars: `GROQ_API_KEY`, `SUPABASE_URL`, `SUPABASE_SERVICE_ROLE_KEY`, and (optional) `GROQ_MODEL` / `SUPABASE_SCHEMA`. Render will run `uvicorn app.main:app --host 0.0.0.0 --port 10000`. Health check: `/healthz`.
- **Frontend (Vercel)**: create a new Vercel project pointing to `frontend/`. Framework: “Other” (no build). Output/public dir: `frontend`. In the UI, set the WebSocket host to your Render URL using `wss://<render-app>.onrender.com`.

### Testing the Flow
1. Send a prompt mentioning “weather”, “pricing”, or “co2” to invoke the LangGraph tool node.
2. Observe Supabase tables filling with a session row plus one event per message.
3. Close the socket to trigger the final summary job; verify the `summary`, `end_time`, and `status` fields update in `sessions`.

### Key Design Choices
- **LangGraph over ad-hoc logic:** The router/tool/prepare nodes keep routing transparent and easily extensible for future tools (e.g., Supabase lookups, vector search).
- **Streaming-first Groq integration:** `GroqStreamer` encapsulates LangChain’s `ChatGroq.astream`, ensuring the WebSocket sends low-latency tokens without blocking the event loop.
- **Async Supabase wrapper:** All DB writes are executed through `asyncio.to_thread` so the synchronous Supabase client never blocks the FastAPI loop.
- **Deterministic automation trigger:** Finalization runs exactly once per disconnect, ensuring summaries and timing metadata remain consistent even if clients exit unexpectedly.

### Next Steps
- Add authentication (JWT or Supabase Auth) to secure session creation.
- Extend `KnowledgeBaseTool` into real function-calling flows (e.g., call an internal REST API).
- Emit Server-Sent Events mirror for analytics dashboards.

